{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse CSPro Dictionary Specification (.DCF) file into dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains example code to parse downloaded DCF files (CSPro format dictionary specification files). This is the first step in MAP processing of DHS data obtained in what they call the \"hierarchical\" format. The DCF files specify a table structure, along with instructions on how to parse the data for these tables out of the data (.DAT) files.\n",
    "\n",
    "This workbook generates \"record spec\", \"Value spec\", and \"relationship spec\" files from the .DCF file. The most important is the record spec: it specifies the tables and columns that are defined in the file. The file has one row per table column, and contains the necessary information to read the .DAT file and for each row in it, work out what table it belongs to and split the data into the appropriate columns. \n",
    "\n",
    "The value spec file specifies for each column of each table, what value or values it may take, and where appropriate, what these values represent if they are somehow coded. \n",
    "\n",
    "The relationship spec file records the information from the \"relations\" section of the DCF. This specifies known or recommended ways of joining tables to produce output (for example) in terms of children, adults, or households. However it doesn't seem to specify all possible links, and so the joiner code can't currently rely entirely on this relationship spec file to construct joins - this part is still experimental and may not be feasible to automate fully.\n",
    "\n",
    "The parsing is implemented in an external file and this workbook contains a simple loop to call the parser for each DCF file in a directory and write out the results to csv files.\n",
    "\n",
    "These CSV files are then suitable as input for the code in the DCF_Parser notebook to instruct it how to parse the actual data files (.dat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# code moved to external file so that FME can also read it\n",
    "from DCF_Parser_Main import parseDCF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse all DCF files in a directory and write spec to CSV (for db import etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inDir = r'\\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\Acquisition\\20160623_Updates\\in'\n",
    "inDir = r'\\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\Acquisition\\All\\437'\n",
    "inDir = r'\\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\Acquisition\\All_MR\\some_manual_mr_downloads_cos_dhs_website_is_shit'\n",
    "inDir = r'\\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\Acquisition\\20170328_Updates'\n",
    "#outDir =  r'\\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\Acquisition\\20160623_Updates\\parsed'\n",
    "outDir = r'\\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\Acquisition\\All\\437'\n",
    "outDir = r'\\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\Acquisition\\All_MR\\parsed'\n",
    "outDir = r'\\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\Acquisition\\20170328_Updates\\parsed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Specify what columns the CSV should have - these must be things that the DCF parser generates\n",
    "reqfieldnames = ['ItemType', 'FileCode','RecordName','RecordTypeValue','RecordLabel','Name','Label',\n",
    "                 'Start','Len','Occurrences','ZeroFill', 'DecimalChar', 'Decimal', 'FMETYPE']\n",
    "valfieldnames = ['FileCode','Name','Value','ValueDesc', 'ValueType']\n",
    "relfieldnames = ['FileCode', 'RelName', 'PrimaryTable', 'PrimaryLink', 'SecondaryTable', 'SecondaryLink']\n",
    "\n",
    "inputDCFs = glob.glob(os.path.join(inDir,'*','*.dcf'))\n",
    "\n",
    "if not os.path.exists(outDir):\n",
    "    os.makedirs(outDir)\n",
    "else:\n",
    "    assert(os.path.isdir(outDir))\n",
    "\n",
    "for inputDCF in inputDCFs:\n",
    "    print inputDCF\n",
    "    # parse it!\n",
    "    parsedDCFItems, parsedDCFRelations = parseDCF(inputDCF, expandRanges=\"All\")\n",
    "    \n",
    "    inBase = os.path.extsep.join(os.path.basename(inputDCF).split(os.path.extsep)[:-1])\n",
    "    outBase = inBase + '.FlatRecordSpec.csv'\n",
    "    outValsBase = inBase + '.FlatValuesSpec.csv'\n",
    "    outRelsBase = inBase + '.RelationshipsSpec.csv'\n",
    "    outFileName = os.path.join(outDir,outBase)\n",
    "    outValsFileName = os.path.join(outDir, outValsBase)\n",
    "    outRelsFileName = os.path.join(outDir, outRelsBase)\n",
    "    \n",
    "    with open(outFileName, 'wb') as fout, open(outValsFileName, 'wb') as fValsOut, \\\n",
    "            open(outRelsFileName, 'wb') as fRelsOut:\n",
    "        wri = csv.writer(fout)\n",
    "        wri.writerow(reqfieldnames)\n",
    "        wriVals = csv.writer(fValsOut)\n",
    "        wriVals.writerow(valfieldnames)\n",
    "        wriRels = csv.writer(fRelsOut)\n",
    "        wriRels.writerow(relfieldnames)\n",
    "        for item in parsedDCFItems:\n",
    "            item['FMETYPE'] = \"fme_char({0!s})\".format(item['Len'])\n",
    "            # write the row using the fieldnames given in reqfieldnames\n",
    "            # not all items have \"occurrences\", \"range_low_value\", etc so write blank value if not\n",
    "            wri.writerow([item[k] if item.has_key(k) else '' for k in reqfieldnames])\n",
    "            # write the value sets to a separate file\n",
    "            if 'Values' in item and len(item['Values'])>0:\n",
    "                vals = item['Values']\n",
    "                for val in vals:\n",
    "                    wriVals.writerow([item['FileCode'],item['Name'], val[0], val[1], val[2]])\n",
    "        for item in parsedDCFRelations:\n",
    "            wriRels.writerow([item[k] if item.has_key(k) else '' for k in relfieldnames])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Not part of the parsing - a bit of code to apply before we do anything else to \n",
    "# rename downloaded files to include survey id number. Assumes they have been downloaded into \n",
    "# subdirectories named with the id number.\n",
    "allFiles = glob.glob(os.path.join(inDir,'*','*'))\n",
    "for fn in allFiles:\n",
    "    if str.lower(fn).find('.zip') != -1:\n",
    "        continue\n",
    "    basename = os.path.basename(fn)\n",
    "    dirname = os.path.dirname(fn)\n",
    "    idname = os.path.basename(dirname)\n",
    "    newname = idname+'.'+basename\n",
    "    newpath = os.path.join(dirname,newname)\n",
    "    #print fn\n",
    "    #print newpath\n",
    "    os.rename(fn,newpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive sql generation of output (not used: use FME if loading to DB; this workbook creates CSVs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of this is actually used - we primarily work off CSVs and then later loaded those CSVs to PG database en bloc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "levelsInserts = []\n",
    "recordsInserts = []\n",
    "itemsInserts = []\n",
    "valuesInserts = []\n",
    "\n",
    "# i think a straightforward bit of sql formatting will do here, nobody malicious will get chance to run...\n",
    "\n",
    "# levels is straightforward, just name and label\n",
    "for name, label in allLevels.iteritems():\n",
    "    insertLevelsSQL = 'INSERT INTO dhs_levels '\\\n",
    "    '(record_name, record_label)' \\\n",
    "    ' VALUES '\\\n",
    "    '(\"{0!s}\", \"{1!s}\");'.format(name, label)\n",
    "    levelsInserts.append(insertLevelsSQL)\n",
    "    \n",
    "# records may be country specific, impute this from the presence of a word followed by \"specific\"\n",
    "# in the label. E.g. \"Country specific\", \"Survey specific\"\n",
    "for name, label in allRecords.iteritems():\n",
    "    m = re.match('^\\w+ specific', label)\n",
    "    if m:\n",
    "        specificText = m.group(0)\n",
    "    else:\n",
    "        specificText = \"No\"\n",
    "        \n",
    "    insertRecordsSQL = 'INSERT INTO dhs_records '\\\n",
    "    '(record_name, record_label, c_or_s_specific)' \\\n",
    "    ' VALUES '\\\n",
    "    '(\"{0!s}\", \"{1!s}\", \"{2!s}\");'.format(name, label, specificText)\n",
    "    recordsInserts.append(insertRecordsSQL)\n",
    "    \n",
    "# items is the main thingy\n",
    "for item in allItems:\n",
    "    insertItemsSQL = 'INSERT INTO dhs_recodes '\\\n",
    "    '(level_id, record_id, recode_id, recode_description, start, len, data_type, item_type, range_low_value, range_high_value)'\\\n",
    "    ' VALUES '\\\n",
    "    '(\"{0!s}\", \"{1!s}\", \"{2!s}\", \"{3!s}\", {4!s}, {5!s}, {6!s}, {7!s}, {8!s}, {9!s});'.format(\n",
    "        item['LevelName'], item['RecordName'], item['Name'], item['Label'], item['Start'], item['Len'], \n",
    "        '\"A\"', '\"B\"', # TODO change these\n",
    "        item['Range_Low_Value'] if item.has_key('Range_Low_Value') else '',\n",
    "        item['Range_High_Value'] if item.has_key('Range_High_Value') else '',\n",
    "    )\n",
    "    itemsInserts.append(insertItemsSQL)\n",
    "    if item.has_key('Values') and len(item['Values'])>0:\n",
    "        for valtuple in item['Values']:\n",
    "            insertValueSQL = 'INSERT INTO dhs_recode_values '\\\n",
    "            '(recode_id, value_code, value_description)' \\\n",
    "            ' VALUES '\\\n",
    "            '(\"{0!s}\", \"{1!s}\", \"{2!s}\");'.format(\n",
    "                item['Name'], valtuple[0], valtuple[1])\n",
    "            valuesInserts.append(insertValueSQL)\n",
    "#print insertSQL\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

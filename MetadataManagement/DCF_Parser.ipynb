{
 "metadata": {
  "name": "",
  "signature": "sha256:d65885013ba7fed3ec6461660a6abaa4b07ff39758ce1d9704ddf245beb6b209"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Parse Recode (.DCF file) into dictionary"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook contains example code to parse downloaded DCF files (CSPro format dictionary specification files). This is the first step in MAP processing of DHS data. The DCF files specify a table structure, along with instructions on how to parse the data for these tables out of the data (.DAT) files.\n",
      "\n",
      "This workbook generates \"record spec\", \"Value spec\", and \"relationship spec\" files from the .DCF file. The most important is the record spec: it specifies the tables and columns that are defined in the file. The file has one row per table column, and contains the necessary information to read the .DAT file and for each row in it, work out what table it belongs to and split the data into the appropriate columns. \n",
      "\n",
      "The value spec file specifies for each column of each table, what value or values it may take, and where appropriate, what these values represent if they are somehow coded. \n",
      "\n",
      "The relationship spec file records the information from the \"relations\" section of the DCF. This specifies known or recommended ways of joining tables to produce output (for example) in terms of children, adults, or households. However it doesn't seem to specify all possible links, and so the joiner code can't currently rely entirely on this relationship spec file to construct joins - this is still experimental.\n",
      "\n",
      "The parsing is implemented in an external file and this workbook contains a simple loop to call the parser for each DCF file in a directory and write out the results to csv files."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      "import re\n",
      "import os\n",
      "import glob\n",
      "import csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# code moved to external file so that FME can also read it\n",
      "from DCF_Parser_Main import parseDCF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#r'C:\\Users\\zool1301\\Documents\\DHS\\RecodeManuals\\Recode6.dcf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Parse all DCF files in a directory and write spec to CSV (for db import etc)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#inDir = r'C:\\Users\\zool1301\\Documents\\DHS\\Acquisition\\20150626_FullSiteScrape\\downloads\\358'\n",
      "#outDir = r'C:\\Users\\zool1301\\Documents\\DHS\\Acquisition\\20150626_FullSiteScrape\\ParsedSpecs'\n",
      "#inDir = r'\\\\129.67.26.176\\map_data\\DHS_Automation\\Acquisition\\20150626_FullSiteScrape\\downloads'\n",
      "#outDir = r'\\\\129.67.26.176\\map_data\\DHS_Automation\\DataExtraction\\20150626_FullSiteScrape\\ParsedSpecs'\n",
      "inDir = r'\\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\Acquisition\\20160623_Updates\\in'\n",
      "outDir =  r'\\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\Acquisition\\20160623_Updates\\parsed'\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Specify what columns the CSV should have - these must be things that the DCF parser generates\n",
      "reqfieldnames = ['ItemType', 'FileCode','RecordName','RecordTypeValue','RecordLabel','Name','Label',\n",
      "                 'Start','Len','Occurrences','ZeroFill', 'DecimalChar', 'Decimal', 'FMETYPE']\n",
      "valfieldnames = ['FileCode','Name','Value','ValueDesc', 'ValueType']\n",
      "relfieldnames = ['FileCode', 'RelName', 'PrimaryTable', 'PrimaryLink', 'SecondaryTable', 'SecondaryLink']\n",
      "#inDir = r'C:\\Users\\zool1301\\Documents\\DHS\\Acquisition\\20150626_FullSiteScrape\\downloads\\358'\n",
      "#outDir = r'C:\\Users\\zool1301\\Documents\\DHS\\Acquisition\\20150626_FullSiteScrape\\ParsedSpecs'\n",
      "#inDir = r'C:\\Users\\zool1301\\Documents\\DHS\\RecodeManuals'\n",
      "#outDir = r'C:\\Users\\zool1301\\Documents\\DHS\\RecodeManuals\\Parsed'\n",
      "\n",
      "inputDCFs = glob.glob(os.path.join(inDir,'*','*.dcf'))\n",
      "\n",
      "if not os.path.exists(outDir):\n",
      "    os.makedirs(outDir)\n",
      "else:\n",
      "    assert(os.path.isdir(outDir))\n",
      "\n",
      "for inputDCF in inputDCFs:\n",
      "    print inputDCF\n",
      "    # parse it!\n",
      "    parsedDCFItems, parsedDCFRelations = parseDCF(inputDCF, expandRanges=\"All\")\n",
      "    \n",
      "    inBase = os.path.extsep.join(os.path.basename(inputDCF).split(os.path.extsep)[:-1])\n",
      "    outBase = inBase + '.FlatRecordSpec.csv'\n",
      "    outValsBase = inBase + '.FlatValuesSpec.csv'\n",
      "    outRelsBase = inBase + '.RelationshipsSpec.csv'\n",
      "    outFileName = os.path.join(outDir,outBase)\n",
      "    outValsFileName = os.path.join(outDir, outValsBase)\n",
      "    outRelsFileName = os.path.join(outDir, outRelsBase)\n",
      "    \n",
      "    with open(outFileName, 'wb') as fout, open(outValsFileName, 'wb') as fValsOut, \\\n",
      "            open(outRelsFileName, 'wb') as fRelsOut:\n",
      "        wri = csv.writer(fout)\n",
      "        wri.writerow(reqfieldnames)\n",
      "        wriVals = csv.writer(fValsOut)\n",
      "        wriVals.writerow(valfieldnames)\n",
      "        wriRels = csv.writer(fRelsOut)\n",
      "        wriRels.writerow(relfieldnames)\n",
      "        for item in parsedDCFItems:\n",
      "            item['FMETYPE'] = \"fme_char({0!s})\".format(item['Len'])\n",
      "            # write the row using the fieldnames given in reqfieldnames\n",
      "            # not all items have \"occurrences\", \"range_low_value\", etc so write blank value if not\n",
      "            wri.writerow([item[k] if item.has_key(k) else '' for k in reqfieldnames])\n",
      "            # write the value sets to a separate file\n",
      "            if 'Values' in item and len(item['Values'])>0:\n",
      "                vals = item['Values']\n",
      "                for val in vals:\n",
      "                    wriVals.writerow([item['FileCode'],item['Name'], val[0], val[1], val[2]])\n",
      "        for item in parsedDCFRelations:\n",
      "            wriRels.writerow([item[k] if item.has_key(k) else '' for k in relfieldnames])\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\Acquisition\\20160623_Updates\\in\\493\\493.KEIR7H.DCF\n",
        "Parsed 10424 lines into 702 items"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\Acquisition\\20160623_Updates\\in\\468\\468.RWIR70.DCF"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Parsed 31290 lines into 1898 items"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\Acquisition\\20160623_Updates\\in\\464\\464.KHIR72.DCF"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Parsed 30683 lines into 1853 items"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\Acquisition\\20160623_Updates\\in\\461\\461.BDIR70.DCF"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Parsed 27582 lines into 1671 items"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\Acquisition\\20160623_Updates\\in\\462\\462.LSIR71.DCF"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Warning, valueset did not seem to match item at line 2132 of file \\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\Acquisition\\20160623_Updates\\in\\462\\462.LSIR71.DCF - please check!\n",
        "Warning, valueset did not seem to match item at line 2150 of file \\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\Acquisition\\20160623_Updates\\in\\462\\462.LSIR71.DCF - please check!\n",
        "Warning, valueset did not seem to match item at line 2939 of file \\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\Acquisition\\20160623_Updates\\in\\462\\462.LSIR71.DCF - please check!\n",
        "Warning, valueset did not seem to match item at line 5765 of file \\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\Acquisition\\20160623_Updates\\in\\462\\462.LSIR71.DCF - please check!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Warning, valueset did not seem to match item at line 25257 of file \\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\Acquisition\\20160623_Updates\\in\\462\\462.LSIR71.DCF - please check!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Parsed 25302 lines into 1551 items\n",
        "\\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\Acquisition\\20160623_Updates\\in\\437\\437.GHIR70.DCF"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Parsed 29865 lines into 1816 items"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Rename files to include survey id number\n",
      "allFiles = glob.glob(os.path.join(inDir,'*','*'))\n",
      "for fn in allFiles:\n",
      "    if str.lower(fn).find('.zip') != -1:\n",
      "        continue\n",
      "    basename = os.path.basename(fn)\n",
      "    dirname = os.path.dirname(fn)\n",
      "    idname = os.path.basename(dirname)\n",
      "    newname = idname+'.'+basename\n",
      "    newpath = os.path.join(dirname,newname)\n",
      "    #print fn\n",
      "    #print newpath\n",
      "    os.rename(fn,newpath)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Naive sql generation of output (not used: use FME if loading to DB; this workbook creates CSVs)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "levelsInserts = []\n",
      "recordsInserts = []\n",
      "itemsInserts = []\n",
      "valuesInserts = []\n",
      "\n",
      "# i think a straightforward bit of sql formatting will do here, nobody malicious will get chance to run...\n",
      "\n",
      "# levels is straightforward, just name and label\n",
      "for name, label in allLevels.iteritems():\n",
      "    insertLevelsSQL = 'INSERT INTO dhs_levels '\\\n",
      "    '(record_name, record_label)' \\\n",
      "    ' VALUES '\\\n",
      "    '(\"{0!s}\", \"{1!s}\");'.format(name, label)\n",
      "    levelsInserts.append(insertLevelsSQL)\n",
      "    \n",
      "# records may be country specific, impute this from the presence of a word followed by \"specific\"\n",
      "# in the label. E.g. \"Country specific\", \"Survey specific\"\n",
      "for name, label in allRecords.iteritems():\n",
      "    m = re.match('^\\w+ specific', label)\n",
      "    if m:\n",
      "        specificText = m.group(0)\n",
      "    else:\n",
      "        specificText = \"No\"\n",
      "        \n",
      "    insertRecordsSQL = 'INSERT INTO dhs_records '\\\n",
      "    '(record_name, record_label, c_or_s_specific)' \\\n",
      "    ' VALUES '\\\n",
      "    '(\"{0!s}\", \"{1!s}\", \"{2!s}\");'.format(name, label, specificText)\n",
      "    recordsInserts.append(insertRecordsSQL)\n",
      "    \n",
      "# items is the main thingy\n",
      "for item in allItems:\n",
      "    insertItemsSQL = 'INSERT INTO dhs_recodes '\\\n",
      "    '(level_id, record_id, recode_id, recode_description, start, len, data_type, item_type, range_low_value, range_high_value)'\\\n",
      "    ' VALUES '\\\n",
      "    '(\"{0!s}\", \"{1!s}\", \"{2!s}\", \"{3!s}\", {4!s}, {5!s}, {6!s}, {7!s}, {8!s}, {9!s});'.format(\n",
      "        item['LevelName'], item['RecordName'], item['Name'], item['Label'], item['Start'], item['Len'], \n",
      "        '\"A\"', '\"B\"', # TODO change these\n",
      "        item['Range_Low_Value'] if item.has_key('Range_Low_Value') else '',\n",
      "        item['Range_High_Value'] if item.has_key('Range_High_Value') else '',\n",
      "    )\n",
      "    itemsInserts.append(insertItemsSQL)\n",
      "    if item.has_key('Values') and len(item['Values'])>0:\n",
      "        for valtuple in item['Values']:\n",
      "            insertValueSQL = 'INSERT INTO dhs_recode_values '\\\n",
      "            '(recode_id, value_code, value_description)' \\\n",
      "            ' VALUES '\\\n",
      "            '(\"{0!s}\", \"{1!s}\", \"{2!s}\");'.format(\n",
      "                item['Name'], valtuple[0], valtuple[1])\n",
      "            valuesInserts.append(insertValueSQL)\n",
      "#print insertSQL\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
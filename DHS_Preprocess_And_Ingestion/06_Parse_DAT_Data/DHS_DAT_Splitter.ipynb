{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DHS (CSPro) .DAT file parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workbook contains sample code for parsing .dat files in the CSPro format used by DHS in their \"hierarchical\" downloads. It splits each .dat file into one CSV file for each table or \"recordtype\" contained in that file.\n",
    "\n",
    "It relies on first having parsed the dictionary specification file associated with a given .dat file(s), generating a \"recordspec\" file. This provides the necessary information for the code in this workbook to read a .dat file and identify how to split the characters found on a given line into individual columns of data associated with a particular table schema.\n",
    "\n",
    "This parsing should therefore be done first (using the code in the DCF_Parser notebook, or similar). At present this notebook uses record spec files in CSV format but of course this could easily be modified to use the specification stuff that has been stored in the database in the dhs_table_specs_flat table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "from operator import itemgetter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inSpecDir = r'\\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\Acquisition\\Update_Staging\\parsed'\n",
    "inDataDir = r'\\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\Acquisition\\Update_Staging\\download'\n",
    "outputCSVTableDir = r'\\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\Acquisition\\Update_Staging\\tables'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDatFiles = glob.glob(os.path.join(inDataDir, '*', '*.dat'))\n",
    "nColsPerRecType = {}\n",
    "# rtFieldInfoAllFiles = {}\n",
    "# resGlobal = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\\\\\\\map-fs1.ndph.ox.ac.uk\\\\map_data\\\\DHS_Automation\\\\Acquisition\\\\Update_Staging\\\\download\\\\500\\\\500.JOMR71.DAT',\n",
       " '\\\\\\\\map-fs1.ndph.ox.ac.uk\\\\map_data\\\\DHS_Automation\\\\Acquisition\\\\Update_Staging\\\\download\\\\500\\\\500.JOIR71.DAT',\n",
       " '\\\\\\\\map-fs1.ndph.ox.ac.uk\\\\map_data\\\\DHS_Automation\\\\Acquisition\\\\Update_Staging\\\\download\\\\523\\\\523.PKMR71.DAT',\n",
       " '\\\\\\\\map-fs1.ndph.ox.ac.uk\\\\map_data\\\\DHS_Automation\\\\Acquisition\\\\Update_Staging\\\\download\\\\523\\\\523.PKIR71.DAT',\n",
       " '\\\\\\\\map-fs1.ndph.ox.ac.uk\\\\map_data\\\\DHS_Automation\\\\Acquisition\\\\Update_Staging\\\\download\\\\494\\\\494.MVIR71.DAT',\n",
       " '\\\\\\\\map-fs1.ndph.ox.ac.uk\\\\map_data\\\\DHS_Automation\\\\Acquisition\\\\Update_Staging\\\\download\\\\494\\\\494.MVMR71.DAT',\n",
       " '\\\\\\\\map-fs1.ndph.ox.ac.uk\\\\map_data\\\\DHS_Automation\\\\Acquisition\\\\Update_Staging\\\\download\\\\390\\\\390.ZADV71FL.DAT',\n",
       " '\\\\\\\\map-fs1.ndph.ox.ac.uk\\\\map_data\\\\DHS_Automation\\\\Acquisition\\\\Update_Staging\\\\download\\\\390\\\\390.ZAIR71.DAT',\n",
       " '\\\\\\\\map-fs1.ndph.ox.ac.uk\\\\map_data\\\\DHS_Automation\\\\Acquisition\\\\Update_Staging\\\\download\\\\390\\\\390.ZAMR71.DAT']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allDatFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should we produce \"global\" outputs e.g. one RECH1 file containing RECH1 from all surveys\n",
    "# as well as the table-by-survey outputs?\n",
    "# This requires a lot of memory!. Probably not required now we are using a db for holding the output; \n",
    "# also it would only make sense if we were parsing all DHS surveys at once. \n",
    "# We'll just store one CSV per survey per record type.\n",
    "doGlobalOutput = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parses all DAT files based on their record specifications, generated separately, into individual CSVs: one per file and per record type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing 500.JOMR71\n",
      "Parsing 500.JOIR71\n",
      "Parsing 523.PKMR71\n",
      "Parsing 523.PKIR71\n",
      "Parsing 494.MVIR71\n",
      "Parsing 494.MVMR71\n",
      "Parsing 390.ZADV71FL\n",
      "Parsing 390.ZAIR71\n",
      "Parsing 390.ZAMR71\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(outputCSVTableDir):\n",
    "    os.makedirs(outputCSVTableDir)\n",
    "else:\n",
    "    assert os.path.isdir(outputCSVTableDir)\n",
    "\n",
    "for datFN in allDatFiles:\n",
    "    # get the corresponding spec file\n",
    "    filecode = os.path.extsep.join(os.path.basename(datFN).split(os.path.extsep)[:-1])\n",
    "    specFileName = os.path.join(inSpecDir, '{0!s}.FlatRecordSpec.csv'.format(filecode))\n",
    "    \n",
    "    # See if we've already done this one\n",
    "    outTestFN = os.path.join(outputCSVTableDir, \n",
    "                             '{0!s}.{1!s}.csv'.format(filecode,'REC01'))\n",
    "    if os.path.exists(outTestFN):\n",
    "        print (\"Already did \" + filecode)\n",
    "        continue\n",
    "    print (\"Parsing \" + filecode)\n",
    "    \n",
    "    # read the variables of this survey, specifying how to split the .dat\n",
    "    with open(specFileName,'r') as dictFile:\n",
    "        dictFileReader = csv.DictReader(dictFile)\n",
    "        # the record type position info must be in the first line\n",
    "        recordTypeInfo = next(dictFileReader)\n",
    "        rtStart = int(recordTypeInfo['Start'])-1\n",
    "        rtEnd = int(recordTypeInfo['Len']) + rtStart\n",
    "        allVarsThisFile = [row for row in dictFileReader]\n",
    "    for fieldInfo in allVarsThisFile:\n",
    "        fieldInfo['Start'] = int(fieldInfo['Start'])\n",
    "        fieldInfo['Len'] = int(fieldInfo['Len'])\n",
    "    # sort them by record type then position in the row\n",
    "    allVarsThisFileSorted = sorted(allVarsThisFile, \n",
    "                                   key=(itemgetter('RecordTypeValue','Start')))\n",
    "    \n",
    "    # build dictionary of record type (tablename) : list of its fields\n",
    "    rtFieldInfoThisFile = {}\n",
    "    for fieldInfo in allVarsThisFileSorted:\n",
    "        recordTag = fieldInfo['RecordTypeValue']\n",
    "        # do it for this file specifically\n",
    "        if recordTag not in rtFieldInfoThisFile:\n",
    "            rtFieldInfoThisFile[recordTag] = []\n",
    "        rtFieldInfoThisFile[recordTag].append(fieldInfo)\n",
    "        \n",
    "        if doGlobalOutput:\n",
    "           # build one across all files that contain this record type\n",
    "           # requires lotsa memory!\n",
    "            if recordTag not in rtFieldInfoAllFiles:\n",
    "                rtFieldInfoAllFiles[recordTag] = {}\n",
    "            fieldName = fieldInfo['Name']\n",
    "            if fieldName not in rtFieldInfoAllFiles[recordTag]:\n",
    "                rtFieldInfoAllFiles[recordTag][fieldName] = fieldInfo\n",
    "            else:\n",
    "                # the position of a given field in the line has to be the same each time\n",
    "                assert rtFieldInfoAllFiles[recordTag][fieldName]['Start'] == fieldInfo['Start']\n",
    "                assert rtFieldInfoAllFiles[recordTag][fieldName]['Len'] == fieldInfo['Len']\n",
    "\n",
    "    # Now parse the survey data file itself\n",
    "    # read the data and put everything into a list, in a dictionary keyed by record type\n",
    "    res = {}\n",
    "    colsPerRecTypeThisFile = {}\n",
    "    with open(datFN, 'r') as data:\n",
    "        linenum = 0\n",
    "        for line in data:\n",
    "            linenum += 1\n",
    "            # the position of the recordtype in the line is the same across the entire file\n",
    "            recordtype = line[rtStart:rtEnd]\n",
    "            # get the spec for this type of row\n",
    "            if recordtype not in rtFieldInfoThisFile:\n",
    "                print (\"Specification for recordtype '{0!s}' not found in file for {1!s} at line {2!s}\".format(\n",
    "                    recordtype, filecode, linenum))\n",
    "                if linenum == 1:\n",
    "                    print (\"As this is the first line of the file, the problem may be that the file is\" +\n",
    "                    \" saved with BOM which skews the byte count. Please re-save the .dat without BOM.\")\n",
    "                continue\n",
    "                \n",
    "            recordSpec = rtFieldInfoThisFile[recordtype]\n",
    "            if recordtype not in res:\n",
    "                res[recordtype] = []\n",
    "            \n",
    "            # split the column-aligned text according to the row specification\n",
    "            # this is the part that is inefficient in FME (lots of list items)\n",
    "            \n",
    "            # The .DAT format allows a fixed width for each column of each recordtype.\n",
    "            # Should we strip the whitespace? This is difficult. In general, yes we should \n",
    "            # but NOT in the case of the CASEID / HHID variables. The HHID is usually the CASEID\n",
    "            # with the last 3 chars trimmed off, but if we trim \"some\" whitespace from HHID here \n",
    "            # then we can break that association and damage referential integrity.\n",
    "            # On the other hand some joins are based on e.g. BIDX (recorded as len 2) \n",
    "            # to MIDX (recorded as len 1, despite containing same data), and we need \n",
    "            # to join on a single digit found in both so BIDX would need to be stripped.\n",
    "            stripornot = lambda data, name: data if name in ('CASEID', 'HHID') else data.strip()\n",
    "            rowParts = [stripornot(\n",
    "                    (line[i['Start']-1 : i['Start']+i['Len']-1]),\n",
    "                    i['Name']) \n",
    "                for i in recordSpec]\n",
    "            \n",
    "            if recordtype not in colsPerRecTypeThisFile:\n",
    "                colsPerRecTypeThisFile[recordtype] = len(rowParts)\n",
    "            else:\n",
    "                assert len(rowParts) == colsPerRecTypeThisFile[recordtype]\n",
    "            # add as a list to the list of rows for this record type\n",
    "            res[recordtype].append(rowParts)#(\",\".join(rowParts))\n",
    "    if doGlobalOutput:\n",
    "        resGlobal[filecode] = res\n",
    "\n",
    "    # write a csv for each record type\n",
    "    for recordtype,fields in rtFieldInfoThisFile.items():\n",
    "        if not recordtype in res:\n",
    "            print (\"No rows found for record type {0!s} in file {1!s} despite DCF specification\".format(\n",
    "                recordtype, filecode))\n",
    "            continue\n",
    "        fieldHeader = [i['Name'] for i in fields]\n",
    "        fieldRecords = set([i['RecordName'] for i in fields])\n",
    "        assert len(fieldRecords) == 1\n",
    "        recName = fieldRecords.pop()\n",
    "        outFN = os.path.join(outputCSVTableDir, '{0!s}.{1!s}.csv'.format(filecode,recName))\n",
    "        with open(outFN, 'w', newline='') as outcsv:\n",
    "            csvwriter = csv.writer(outcsv)\n",
    "            csvwriter.writerow(fieldHeader)\n",
    "            csvwriter.writerows(res[recordtype])\n",
    "\n",
    "if doGlobalOutput:\n",
    "    # write one file for each table. Each output table must have the unioned set of columns \n",
    "    # from each survey's version of this table.\n",
    "    for filecode, fileres in resGlobal.iteritems():\n",
    "        for recordtype,fields in rtFieldInfoAllFiles.items():\n",
    "            allFieldsThisRecordSorted = sorted(fields, key=(itemgetter('Start')))\n",
    "            groupFieldHeader = [i['Name'] for i in allFieldsThisRecordSorted]\n",
    "            fieldRecords = set([i['RecordName'] for i in fields])\n",
    "            assert len(fieldRecords) == 1\n",
    "            recName = fieldRecords.pop()\n",
    "            outGroupFN = os.path.join(outputCSVTableDir, '{0!s}_All.csv'.format(recName))\n",
    "            if not os.path.exists(outGroupFN):\n",
    "                # create the output file for this table\n",
    "                with open(outGroupFN, 'w', newline='') as outcsv:\n",
    "                    csvwriter = csv.writer(outcsv)\n",
    "                    groupFieldHeader.insert(0,'FileCode')\n",
    "                    groupFieldHeader = [i.lower() for i in groupFieldHeader]\n",
    "                    csvwriter.writerow(groupFieldHeader)\n",
    "                    colsPerRecType[recordtype]=len(groupFieldHeader)\n",
    "               \n",
    "            [i.insert(0,filecode) for i in fileres[recordtype]]\n",
    "            with open(outGroupFN, 'a', newline='') as outcsv:\n",
    "                csvwriter = csv.writer(outcsv)\n",
    "                resThisFileThisRecord = fileres[recordtype]\n",
    "                allSameLength = True\n",
    "                for i in resThisFileThisRecord:\n",
    "                    if len(i) != groupFieldHeader:\n",
    "                        allSameLength = False\n",
    "                        break\n",
    "                if allSameLength:\n",
    "                    csvwriter.writerows(resThisFileThisRecord)\n",
    "                else:\n",
    "                    for i in res:\n",
    "                        pass\n",
    "                        # Todo - write this \n",
    "\n",
    "                if len(res[recordtype][0]) != colsPerRecType[recordtype]:\n",
    "                    print (\"Warning! File {0!s} record type {1!s} has more cols than were defined in an earlier file\"\n",
    "                           .format(filecode, recordtype))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

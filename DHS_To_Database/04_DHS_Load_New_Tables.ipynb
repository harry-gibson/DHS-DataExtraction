{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config/pg_conn.txt') as conn_details:\n",
    "    conn_str_psyco = conn_details.readline()\n",
    "    conn_str_sqlalchemy = conn_details.readline()\n",
    "\n",
    "import pandas as pd\n",
    "import pandas.io.sql as psql\n",
    "import psycopg2 as pg\n",
    "from sqlalchemy import create_engine\n",
    "import sqlalchemy as sa\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(conn_str_sqlalchemy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEC_SCHEMA = 'dhs_survey_specs'\n",
    "DATA_SCHEMA = 'dhs_data_tables'\n",
    "\n",
    "#TABLESPEC_TABLENAME = 'table_specs_blank' #'dhs_table_specs_flat'\n",
    "TABLESPEC_TABLENAME = 'dhs_table_specs_flat'\n",
    "#VALUESPEC_TABLENAME = 'value_specs_blank' #'dhs_value_descs'\n",
    "VALUESPEC_TABLENAME = 'dhs_value_descs'\n",
    "#SURVEYLIST_TABLENAME = 'dhs_survey_listing_backup'\n",
    "SURVEYLIST_TABLENAME = 'dhs_survey_listing'\n",
    "\n",
    "TABLE_SPEC_TABLE = \".\".join((SPEC_SCHEMA, TABLESPEC_TABLENAME))\n",
    "VALUE_SPEC_TABLE = \".\".join((SPEC_SCHEMA, VALUESPEC_TABLENAME))\n",
    "SURVEYLIST_TABLE = \".\".join((SPEC_SCHEMA, SURVEYLIST_TABLENAME))\n",
    "\n",
    "STAGING_FOLDER = \"/mnt/c/Users/harry/OneDrive - Nexus365/Informal_Cities/DHS_Data_And_Prep/Staging\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"REC01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = engine.execute(\"\"\"SELECT\n",
    "    tablename,\n",
    "    indexname,\n",
    "    indexdef\n",
    "FROM\n",
    "    pg_indexes\n",
    "WHERE\n",
    "    schemaname = 'dhs_data_tables'\n",
    "ORDER BY\n",
    "    tablename,\n",
    "    indexname;\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_helper = TableDataHelper(conn_str=conn_str_sqlalchemy, table_spec_table=TABLESPEC_TABLENAME,\n",
    "                             value_spec_table=VALUESPEC_TABLENAME, spec_schema=SPEC_SCHEMA,\n",
    "                           data_schema=DATA_SCHEMA, dry_run=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_helper._does_data_table_exist(\"RECHEM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_helper._is_dry_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_helper._is_dry_run = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped existing index surveyid_rechem\n",
      "Skipped existing index hhid_rechem\n",
      "Skipped existing index idxhe_rechem\n",
      "Skipped existing covering index allidx_rechem\n",
      "Skipped existing secondary covering index twoidx_rechem\n",
      "Would create table with \n",
      "\n",
      "            CREATE TABLE dhs_data_tables.\"RECHEM\"(surveyid character varying(3) COLLATE pg_catalog.\"default\",\n",
      "hhid character varying(12) COLLATE pg_catalog.\"default\",\n",
      "idxhe character varying(2) COLLATE pg_catalog.\"default\",\n",
      "sh63 character varying(1) COLLATE pg_catalog.\"default\",\n",
      "sh64 character varying(1) COLLATE pg_catalog.\"default\",\n",
      "sh65 character varying(1) COLLATE pg_catalog.\"default\",\n",
      "sh66 character varying(1) COLLATE pg_catalog.\"default\",\n",
      "sh67 character varying(1) COLLATE pg_catalog.\"default\",\n",
      "sh68 character varying(2) COLLATE pg_catalog.\"default\",\n",
      "sh69 character varying(3) COLLATE pg_catalog.\"default\",\n",
      "sh70 character varying(3) COLLATE pg_catalog.\"default\",\n",
      "sh71 character varying(1) COLLATE pg_catalog.\"default\",\n",
      "sh72 character varying(1) COLLATE pg_catalog.\"default\",\n",
      "sh73 character varying(1) COLLATE pg_catalog.\"default\",\n",
      "sh74 character varying(1) COLLATE pg_catalog.\"default\",\n",
      "sh75 character varying(1) COLLATE pg_catalog.\"default\")\n",
      "            TABLESPACE pg_default;\n",
      "            ALTER TABLE dhs_data_tables.\"RECHEM\" OWNER to admin;\n",
      " then create indices with \n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = db_helper.create_data_table(\"RECHEM\")\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = engine.execute(f'SELECT * FROM dhs_data_tables.\"RECH1\" LIMIT 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>maxlen</th>\n",
       "      <th>start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aidsex</td>\n",
       "      <td>1</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>caseid</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name  maxlen  start\n",
       "0  aidsex       1    157\n",
       "1  caseid      18      1"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql(f\"\"\"\n",
    "            SELECT LOWER(name) AS name, MAX(len) AS maxlen, MAX(start) as start\n",
    "            FROM {TABLE_SPEC_TABLE}\n",
    "            WHERE recordname = '{table_name}' AND lower(name) LIKE '%id%'\n",
    "            GROUP BY name\"\"\", con=engine)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "whats_needed.loc[-1] = ('surveyid', 'Item', '3', 0)\n",
    "whats_needed.index = whats_needed.index + 1  # shifting index\n",
    "whats_needed.sort_index(inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "whats_needed.loc[len(whats_needed)] = ('data', 'Item', '', 999999999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_specs = pd.read_sql(f\"\"\"\n",
    "            SELECT \n",
    "\t            lower(s.name) AS name, MAX(s.len) as req_len, \n",
    "\t            MAX(i.character_maximum_length) as actual_len\n",
    "            FROM \n",
    "                {TABLE_SPEC_TABLE} s\n",
    "            INNER JOIN \n",
    "                information_schema.columns i\n",
    "            ON \n",
    "                s.recordname = i.table_name \n",
    "                AND \n",
    "                'dhs_data_tables'=i.table_schema \n",
    "                AND \n",
    "                lower(s.name)=i.column_name\n",
    "            WHERE s.recordname='MREC01'\n",
    "            GROUP BY s.name\"\"\"\n",
    "        , con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_widen = length_specs[length_specs['req_len']>length_specs['actual_len']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = os.path.join(STAGING_FOLDER, \"tables\", \"248.ETIR51.RECH4.csv\")\n",
    "#test = pd.read_csv(test_file, converters=defaultdict(lambda i: str)).fillna('')\n",
    "test = pd.read_csv(test_file, dtype=str).fillna('')\n",
    "test.columns = test.columns.str.lower()\n",
    "test['surveyid'] = '248'\n",
    "test = test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>hml12</th>\n",
       "      <th>hml13</th>\n",
       "      <th>hml14</th>\n",
       "      <th>hml15</th>\n",
       "      <th>hml16</th>\n",
       "      <th>hml17</th>\n",
       "      <th>hml18</th>\n",
       "      <th>hml19</th>\n",
       "      <th>sh03</th>\n",
       "      <th>sh10</th>\n",
       "      <th>sh11</th>\n",
       "      <th>sh12</th>\n",
       "      <th>sh13</th>\n",
       "      <th>sh14</th>\n",
       "      <th>sh15</th>\n",
       "      <th>sh16</th>\n",
       "      <th>sh17</th>\n",
       "      <th>sh18</th>\n",
       "      <th>sh19</th>\n",
       "      <th>sh20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hhid</th>\n",
       "      <th>idxh4</th>\n",
       "      <th>surveyid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1 17</th>\n",
       "      <th>1</th>\n",
       "      <th>248</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>248</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>248</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>248</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <th>248</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            hml12 hml13 hml14 hml15 hml16 hml17 hml18 hml19  \\\n",
       "hhid         idxh4 surveyid                                                   \n",
       "        1 17 1     248          0     0                26     1           0   \n",
       "             2     248          0     0                22     1     0     0   \n",
       "             3     248          0     0                 5     0           0   \n",
       "             4     248          0     0                 1     0           0   \n",
       "             5     248          0     0                18     1           0   \n",
       "\n",
       "                            sh03 sh10 sh11 sh12 sh13 sh14 sh15 sh16 sh17 sh18  \\\n",
       "hhid         idxh4 surveyid                                                     \n",
       "        1 17 1     248         1         0         0    0                       \n",
       "             2     248         2         0         0    0                       \n",
       "             3     248         3    1    2    1    1    0                       \n",
       "             4     248         3    1    2    1    1                            \n",
       "             5     248         8         0         0    1    5    1    5    1   \n",
       "\n",
       "                            sh19 sh20  \n",
       "hhid         idxh4 surveyid            \n",
       "        1 17 1     248                 \n",
       "             2     248                 \n",
       "             3     248                 \n",
       "             4     248              3  \n",
       "             5     248         4       "
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.set_index([c for c in test.columns if 'id' in c], inplace=True)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = engine.raw_connection()\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer = io.StringIO()\n",
    "test.to_csv(buffer, sep='\\t', header=False, index=False)\n",
    "buffer.seek(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hhid</th>\n",
       "      <th>idxh4</th>\n",
       "      <th>surveyid</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 17</td>\n",
       "      <td>1</td>\n",
       "      <td>248</td>\n",
       "      <td>{\"hml12\": \"0\", \"hml13\": \"0\", \"hml14\": \"\", \"hml...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 17</td>\n",
       "      <td>2</td>\n",
       "      <td>248</td>\n",
       "      <td>{\"hml12\": \"0\", \"hml13\": \"0\", \"hml14\": \"\", \"hml...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 17</td>\n",
       "      <td>3</td>\n",
       "      <td>248</td>\n",
       "      <td>{\"hml12\": \"0\", \"hml13\": \"0\", \"hml14\": \"\", \"hml...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 17</td>\n",
       "      <td>4</td>\n",
       "      <td>248</td>\n",
       "      <td>{\"hml12\": \"0\", \"hml13\": \"0\", \"hml14\": \"\", \"hml...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 17</td>\n",
       "      <td>5</td>\n",
       "      <td>248</td>\n",
       "      <td>{\"hml12\": \"0\", \"hml13\": \"0\", \"hml14\": \"\", \"hml...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           hhid idxh4 surveyid  \\\n",
       "0          1 17     1      248   \n",
       "1          1 17     2      248   \n",
       "2          1 17     3      248   \n",
       "3          1 17     4      248   \n",
       "4          1 17     5      248   \n",
       "\n",
       "                                                data  \n",
       "0  {\"hml12\": \"0\", \"hml13\": \"0\", \"hml14\": \"\", \"hml...  \n",
       "1  {\"hml12\": \"0\", \"hml13\": \"0\", \"hml14\": \"\", \"hml...  \n",
       "2  {\"hml12\": \"0\", \"hml13\": \"0\", \"hml14\": \"\", \"hml...  \n",
       "3  {\"hml12\": \"0\", \"hml13\": \"0\", \"hml14\": \"\", \"hml...  \n",
       "4  {\"hml12\": \"0\", \"hml13\": \"0\", \"hml14\": \"\", \"hml...  "
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'        1 17\\t1\\t248\\t\"{\"\"hml12\"\": \"\"0\"\", \"\"hml13\"\": \"\"0\"\", \"\"hml14\"\": \"\"\"\", \"\"hml15\"\": \"\"\"\", \"\"hml16\"\": \"\"26\"\", \"\"hml17\"\": \"\"1\"\", \"\"hml18\"\": \"\"\"\", \"\"hml19\"\": \"\"0\"\", \"\"sh03\"\": \"\"1\"\", \"\"sh10\"\": \"\"\"\", \"\"sh11\"\": \"\"0\"\", \"\"sh12\"\": \"\"\"\", \"\"sh13\"\": \"\"0\"\", \"\"sh14\"\": \"\"0\"\", \"\"sh15\"\": \"\"\"\", \"\"sh16\"\": \"\"\"\", \"\"sh17\"\": \"\"\"\", \"\"sh18\"\": \"\"\"\", \"\"sh19\"\": \"\"\"\", \"\"sh20\"\": \"\"\"\"}\"\\n'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.copy_from(buffer, DATA_SCHEMA+\".\"+'\"TEST_RH4\"', sep='\\t', columns=list(test.columns))\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_sql(name='TEST_RH4', schema=DATA_SCHEMA, con=engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as sa\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "class TableDataHelper:\n",
    "    _MAX_COLUMN_THRESHOLD = 1000\n",
    "    def __init__(self, conn_str, \n",
    "        table_spec_table, value_spec_table, spec_schema,\n",
    "        data_schema, dry_run=True):\n",
    "        self._engine = create_engine(conn_str)\n",
    "        self._TABLE_SPEC_TABLENAME = table_spec_table\n",
    "        self._VALUE_SPEC_TABLENAME = value_spec_table\n",
    "        self._SPEC_SCHEMA = spec_schema\n",
    "        self._TABLE_SPEC_TABLE = \".\".join([spec_schema, table_spec_table])\n",
    "        self._VALUE_SPEC_TABLE = \".\".join([spec_schema, value_spec_table])\n",
    "        self._DATA_SCHEMA = data_schema\n",
    "        self._is_dry_run = dry_run\n",
    "        \n",
    "        self._populate_JSON_table_list()\n",
    "        \n",
    "        self._known_tables = set()\n",
    "        self._checked_column_tables = set()\n",
    "\n",
    "    \n",
    "    def _populate_JSON_table_list(self):\n",
    "        json_tables = pd.read_sql(f\"\"\"\n",
    "            SELECT DISTINCT table_name \n",
    "            FROM information_schema.columns \n",
    "            WHERE table_schema = '{self._DATA_SCHEMA}' AND data_type = 'jsonb'\"\"\",\n",
    "                con=self._engine)\n",
    "        self._json_tables = set(json_tables['table_name'])\n",
    "\n",
    "    def _does_data_table_exist(self, table_name):\n",
    "        if table_name in self._known_tables:\n",
    "            return True\n",
    "        exists = pd.read_sql(f\"\"\"\n",
    "            SELECT EXISTS (\n",
    "                SELECT FROM information_schema.tables \n",
    "                WHERE  table_schema = '{self._DATA_SCHEMA}'\n",
    "                AND    table_name   = '{table_name}'\n",
    "            );\"\"\", con=self._engine)['exists'][0]\n",
    "        if exists:\n",
    "            self._known_tables.add(table_name)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    def prepare_db_for_file(self, table_name):\n",
    "        if not self._does_data_table_exist(table_name):\n",
    "            self.create_data_table(table_name)\n",
    "        else:\n",
    "            self.check_cols_against_metadata(table_name)\n",
    "\n",
    "\n",
    "    def create_data_table(self, table_name):\n",
    "        \"\"\"Creates a data table with all the columns that are currently specified in the metadata.\n",
    "        \n",
    "        As throughout this software, all columns are created with VARCHAR(n) type where n = the \n",
    "        maximum width currently specified for that table/column in any survey\n",
    "        \n",
    "        The exception if there are more than TableDataHelper._MAX_COLUMN_THRESHOLD columns \n",
    "        specified in the metadata, in which case only columns with \"id\" in the name will be \n",
    "        first-class columns, and a jsonb column named data will be added for storing the remainder.\"\"\"\n",
    "        column_clauses, is_json = self._get_column_clauses(table_name)\n",
    "        create_stmt = f\"\"\"\n",
    "            CREATE TABLE {self._DATA_SCHEMA}.\"{table_name}\"({column_clauses})\n",
    "            TABLESPACE pg_default;\n",
    "            ALTER TABLE {self._DATA_SCHEMA}.\"{table_name}\" OWNER to admin;\"\"\"\n",
    "        if self._is_dry_run:\n",
    "            return (\"Would create table with \\n\" + create_stmt + \"\\n\" \n",
    "            + \" then create indices with \" + self._create_or_replace_indices(table_name, is_json))\n",
    "        else:\n",
    "            r = self._engine.execute(create_stmt)\n",
    "            if is_json:\n",
    "                self._json_tables.add(table_name)\n",
    "            index_sql = self._create_or_replace_indices(table_name, is_json)\n",
    "            if len(index_sql) > 0:\n",
    "                print(\"Executing the following to drop / recreate indices: \\n\" + index_sql)\n",
    "                self._engine.execute(index_sql)\n",
    "        \n",
    "\n",
    "    def _get_column_clauses(self, table_name):\n",
    "        \"\"\"Gets the columns that a new data table should have, according to the metadata tables.\n",
    "        \n",
    "        Returns them as a string for use in a statement of the form \n",
    "        CREATE TABLE tablename (result) \"\"\"\n",
    "        \n",
    "        # list all columns that are specified for this datatable in the survey metadata (unioned \n",
    "        # set across all surveys: not all surveys will have all columns)\n",
    "        whats_needed = pd.read_sql(f\"\"\"\n",
    "            SELECT lower(name) AS name, itemtype, MAX(len) AS length, MAX(start) AS start\n",
    "            FROM {self._TABLE_SPEC_TABLE}\n",
    "            WHERE recordname='{table_name}'\n",
    "            GROUP BY name, itemtype\n",
    "            ORDER BY start;\n",
    "        \"\"\", con=self._engine)\n",
    "        \n",
    "        # In the case of some country-specific tables, where the columns are different in almost \n",
    "        # every survey, the number of columns becomes very large and horribly inefficient to store \n",
    "        # (it is sparse). In these cases we store those tables with a single JSONB column for the data \n",
    "        # plus the ID/joining columns as first-class columns\n",
    "        is_json = False\n",
    "        if len(whats_needed) > TableDataHelper._MAX_COLUMN_THRESHOLD:\n",
    "            whats_needed = whats_needed[whats_needed['name'].str.contains('id')]\n",
    "            whats_needed.loc[len(whats_needed)] = ('data', 'JSON', '', 99999999)\n",
    "            is_json=True\n",
    "\n",
    "        # of course the metadata tables don't specify surveyid so we add that manually\n",
    "        whats_needed.loc[-1] = ('surveyid', 'Item', '3', 0)\n",
    "        whats_needed.index = whats_needed.index + 1\n",
    "        whats_needed.sort_index(inplace=True)\n",
    "        \n",
    "        def _clause_from_row(r):\n",
    "            if r[\"itemtype\"] == \"JSON\":\n",
    "                type_clause = \"jsonb\"\n",
    "            else:\n",
    "                type_clause = f'character varying({r[\"length\"]})'\n",
    "            clause = f'{r[\"name\"]} {type_clause} COLLATE pg_catalog.\"default\"'\n",
    "            return clause \n",
    "\n",
    "        # convert each row in the df to a clause for use in the CREATE TABLE statement\n",
    "        clauses = list(whats_needed.apply(_clause_from_row, axis=1))\n",
    "        return (\",\\n\".join(clauses), is_json)\n",
    "\n",
    "\n",
    "    def _create_or_replace_indices(self, table_name, is_json=False, replace_existing=False):\n",
    "        # TODO create GIN index on any JSON columns?\n",
    "        idx_sql_template = 'CREATE INDEX {0} ON {1}.\"{2}\"({3});'\n",
    "        idx_name_template = '{0}_{1}'\n",
    "        clean_sql_template = 'DROP INDEX IF EXISTS {0}.{1};'\n",
    "\n",
    "        res = self._engine.execute(\"SELECT relname FROM pg_class WHERE relkind='i';\")\n",
    "        existing_indices = [i[0] for i in res.fetchall()]\n",
    "\n",
    "        tbl_cols = self._engine.execute(f'SELECT * FROM dhs_data_tables.\"{table_name}\" LIMIT 0').keys()\n",
    "        idx_fields = [c for c in tbl_cols if c.find('id') != -1]\n",
    "\n",
    "        drop_idx_stmts = []\n",
    "        idx_stmts = []\n",
    "\n",
    "        for c in idx_fields:\n",
    "            idx_name = idx_name_template.format(c, str.lower(table_name))\n",
    "            idx_sql = idx_sql_template.format(idx_name, self._DATA_SCHEMA, table_name, c)\n",
    "            if idx_name in existing_indices:\n",
    "                if replace_existing:\n",
    "                    drop_idx_stmt = clean_sql_template.format(self._DATA_SCHAME, idx_name)\n",
    "                    drop_idx_stmts.append(drop_idx_stmt)\n",
    "                    idx_stmts.append(idx_sql)\n",
    "                    print(\"Replacing index \" + idx_name)\n",
    "                else:\n",
    "                    print(\"Skipped existing index \" + idx_name)\n",
    "            else:\n",
    "                idx_stmts.append(idx_sql)\n",
    "                print(\"Adding index \"+idx_name)\n",
    "        \n",
    "        # also create a single covering index on all joining columns\n",
    "        if len(idx_fields) > 1:\n",
    "            idx_name = idx_name_template.format(\"allidx\", str.lower(table_name))\n",
    "            idx_sql = idx_sql_template.format(idx_name, self._DATA_SCHEMA, table_name, \",\".join(idx_fields))\n",
    "            if idx_name in existing_indices:\n",
    "                if replace_existing:\n",
    "                    drop_idx_stmt = clean_sql_template.format(self._DATA_SCHEMA, idx_name)\n",
    "                    drop_idx_stmts.append(drop_idx_stmt)\n",
    "                    idx_stmts.append(idx_sql)\n",
    "                    print(\"Replacing covering index \" + idx_name)\n",
    "                else:\n",
    "                    print(\"Skipped existing covering index \" + idx_name)\n",
    "            else:\n",
    "                idx_stmts.append(idx_sql)\n",
    "                print(\"Adding covering index \"+idx_name)\n",
    "        \n",
    "        # also create a covering index on the first two joining columns if there are three \n",
    "        # (or all except the last one, if there's more)\n",
    "        # e.g. surveyid and caseid but not bidx (the cols are in the appropriate order in the CSVs)\n",
    "        if len(idx_fields) > 2:\n",
    "            idx_name = idx_name_template.format(\"twoidx\", str.lower(table_name))\n",
    "            idx_sql = idx_sql_template.format(idx_name, self._DATA_SCHEMA,\n",
    "                                            table_name, \",\".join(idx_fields[:-1]))\n",
    "            if idx_name in existing_indices:\n",
    "                if replace_existing:\n",
    "                    drop_idx_stmt = clean_sql_template.format(self._DATA_SCHEMA, idx_name)\n",
    "                    drop_idx_stmts.append(drop_idx_stmt)\n",
    "                    idx_stmts.append(idx_sql)\n",
    "                    print (\"Replacing secondary covering index \" + idx_name)\n",
    "                else:\n",
    "                    print (\"Skipped existing secondary covering index \" + idx_name)\n",
    "            else:\n",
    "                idx_stmts.append(idx_sql)\n",
    "                print (\"Adding secondary covering index \" + idx_name)\n",
    "        \n",
    "        drop_indices_sql = \"\\n\".join(drop_idx_stmts)\n",
    "        create_indices_sql = \"\\n\".join(idx_stmts)\n",
    "\n",
    "        return drop_indices_sql + \"\\n\" + create_indices_sql\n",
    "  \n",
    "\n",
    "    def get_db_survey_table_rowcount(self, surveyid, tablename):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def check_and_update_column_width(self, schema_name, table_name, column_name, \n",
    "                                req_width, update_if_needed=False):\n",
    "        \"\"\"Check that a varchar(n) column in the database is at least req_width wide and widen it if not\"\"\"\n",
    "        df = pd.read_sql(f'''\n",
    "            SELECT character_maximum_length as maxlen \n",
    "            FROM information_schema.columns \n",
    "            WHERE table_name = '{table_name}' and column_name = '{column_name}';''', con=self._engine)\n",
    "        current = df['maxlen'][0]\n",
    "        if current < req_width and update_if_needed:\n",
    "            print(f\"Column {schema_name}.{table_name}.{column_name} will be widened from\"+\n",
    "                \"{current} to {req_width}\")\n",
    "            sql = f'''ALTER TABLE {schema_name}.\"{table_name}\" \n",
    "                        ALTER COLUMN {column_name} TYPE character varying({req_width})'''\n",
    "            return self._engine.execute(sql)\n",
    "        else:\n",
    "            return current >= req_width\n",
    "\n",
    "\n",
    "    def check_destination_col_widths(self, df, schema_name, table_name):\n",
    "        \"\"\"Check that for all object columns in the provided dataframe, the equivalent column \n",
    "        in the database is at least wide enough to hold it, and widen it if not.\"\"\"\n",
    "        obj_cols = df.select_dtypes(include=['object']).columns\n",
    "        lengths = {col:(df[col].str.len().max()) for col in obj_cols}\n",
    "        for col, incoming_length in lengths.items():\n",
    "            self.check_and_update_column_width(schema_name, table_name, col, incoming_length, True)\n",
    "\n",
    "    \n",
    "    def check_missing_cols(self, table_name):\n",
    "        \"\"\"For a given data table, check the metadata tables to see what all the columns \n",
    "        needed are, and then check whether they all exist. Returns a dataframe of column names \n",
    "        that are missing from the table, if the table is not one with a JSONB column.\"\"\"\n",
    "         \n",
    "        is_json = table_name in self._json_tables\n",
    "\n",
    "        if is_json:\n",
    "            data_cols_needed = pd.read_sql(f\"\"\"\n",
    "                SELECT LOWER(name) AS name, MAX(len) AS maxlen, MAX(start) as start\n",
    "                FROM {self._TABLE_SPEC_TABLE}\n",
    "                WHERE recordname = '{table_name}' AND lower(name) LIKE '%id%'\n",
    "                GROUP BY name\"\"\", con=self._engine)\n",
    "            data_cols_needed.loc[len(data_cols_needed)] = ('data', 99999999, 99999999)\n",
    "\n",
    "        else:\n",
    "            data_cols_needed = pd.read_sql(f\"\"\"\n",
    "                SELECT LOWER(name) AS name, MAX(len) AS maxlen \n",
    "                FROM {self._TABLE_SPEC_TABLE}\n",
    "                WHERE recordname = '{table_name}'\n",
    "                GROUP BY name\n",
    "                ORDER BY name, start\"\"\", con=self._engine)\n",
    "            \n",
    "        data_cols_present = pd.read_sql(f\"\"\"\n",
    "            SELECT table_name, column_name, data_type \n",
    "            FROM information_schema.columns \n",
    "            WHERE table_schema = '{self._DATA_SCHEMA}' ORDER BY table_name, column_name\"\"\", \n",
    "            con=self._engine)\n",
    "        \n",
    "        data_cols_not_present = data_cols_needed[~data_cols_needed['name'].isin(\n",
    "            data_cols_present['column_name'])]\n",
    "\n",
    "        #and_not_in_json_tbl = data_cols_not_present[~data_cols_not_present['recordname'].isin(\n",
    "        #    self._json_tables)]\n",
    "        return data_cols_not_present\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "    def add_varchar_col_to_table(self, schema_name, table_name, column_name, req_width):\n",
    "        sql = f\"\"\"\n",
    "            ALTER TABLE {schema_name}.\"{table_name}\" \n",
    "            ADD COLUMN {column_name.lower()} CHARACTER VARYING ({req_width})\"\"\"\n",
    "        return engine.execute(sql)\n",
    "\n",
    "\n",
    "    def delete_table_entries_for_survey(surveyid, table_name, dry_run=True):\n",
    "        # only drop ones for matching columns because we get different ones from the IR and MR files\n",
    "        if dry_run:\n",
    "            print(f\"Would drop all data rows for survey {surveyid} from {table_name}\")\n",
    "        else:\n",
    "            print(f\"Dropping all data rows for survey {surveyid} from {table_name}\")\n",
    "            meta = sa.MetaData()\n",
    "            tablespec_db = sa.Table(table_name, meta, schema=self._DATA_SCHEMA, \n",
    "                                    autoload=True, autoload_with=engine)\n",
    "            cond = tablespec_db.c.surveyid == surveyid\n",
    "            delete = tablespec_db.delete().where(cond)  \n",
    "            res = engine.execute(delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "metadata": {
  "name": "",
  "signature": "sha256:3a7a1cbe0777466f33cb1feb972ade28617ede098315f69395bf5b406a18cde9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "import glob\n",
      "from operator import itemgetter\n",
      "import os"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "inSpecDir = r'\\\\129.67.26.176\\map_data\\DHS_Automation\\DataExtraction\\All\\ParsedSpecs'\n",
      "inDataDir = r'\\\\129.67.26.176\\map_data\\DHS_Automation\\Acquisition\\All'\n",
      "outputCSVTableDir = r'\\\\129.67.26.176\\map_data\\DHS_Automation\\DataExtraction\\All\\ProcessedTables'\n",
      "\n",
      "inSpecDir = r'\\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\Acquisition\\20160623_Updates\\parsed'\n",
      "inDataDir = r'\\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\Acquisition\\20160623_Updates\\in'\n",
      "outputCSVTableDir = r'\\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\Acquisition\\20160623_Updates\\tables'\n",
      "#inSpecDir = r'\\\\129.67.26.176\\map_data\\DHS_Automation\\DataExtraction\\20160307_Updates\\ParsedSpecs'\n",
      "#inDataDir = r'\\\\129.67.26.176\\map_data\\DHS_Automation\\Acquisition\\20160307_Updates\\downloads'\n",
      "#outputCSVTableDir = r'\\\\129.67.26.176\\map_data\\DHS_Automation\\DataExtraction\\20160307_Updates\\ProcessedTables_PartTrimmed'\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "allDatFiles = glob.glob(os.path.join(inDataDir, '*', '*.dat'))\n",
      "nColsPerRecType = {}\n",
      "# rtFieldInfoAllFiles = {}\n",
      "# resGlobal = {}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "allDatFiles"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Should we produce \"global\" outputs e.g. one RECH1 file containing RECH1 from all surveys\n",
      "# as well as the table-by-survey outputs?\n",
      "# This requires a lot of memory!\n",
      "doGlobalOutput = False"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Parses all DAT files based on their record specifications, generated separately, into individual CSVs: one per file and per record type"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if not os.path.exists(outputCSVTableDir):\n",
      "    os.makedirs(outputCSVTableDir)\n",
      "else:\n",
      "    assert os.path.isdir(outputCSVTableDir)\n",
      "\n",
      "for datFN in allDatFiles:\n",
      "    # get the corresponding spec file\n",
      "    filecode = os.path.extsep.join(os.path.basename(datFN).split(os.path.extsep)[:-1])\n",
      "    specFileName = os.path.join(inSpecDir, '{0!s}.FlatRecordSpec.csv'.format(filecode))\n",
      "    \n",
      "    # See if we've already done this one\n",
      "    outTestFN = os.path.join(outputCSVTableDir, \n",
      "                             '{0!s}.{1!s}.csv'.format(filecode,'REC01'))\n",
      "    if os.path.exists(outTestFN):\n",
      "        print \"Already did \"+filecode\n",
      "        continue\n",
      "    print \"Parsing \"+filecode\n",
      "    \n",
      "    # read the variables of this survey, specifying how to split the .dat\n",
      "    with open(specFileName,'r') as dictFile:\n",
      "        dictFileReader = csv.DictReader(dictFile)\n",
      "        # the record type position info must be in the first line\n",
      "        recordTypeInfo = dictFileReader.next()\n",
      "        rtStart = int(recordTypeInfo['Start'])-1\n",
      "        rtEnd = int(recordTypeInfo['Len']) + rtStart\n",
      "        allVarsThisFile = [row for row in dictFileReader]\n",
      "    for fieldInfo in allVarsThisFile:\n",
      "        fieldInfo['Start'] = int(fieldInfo['Start'])\n",
      "        fieldInfo['Len'] = int(fieldInfo['Len'])\n",
      "    # sort them by record type then position in the row\n",
      "    allVarsThisFileSorted = sorted(allVarsThisFile, \n",
      "                                   key=(itemgetter('RecordTypeValue','Start')))\n",
      "    \n",
      "    # build dictionary of record type (tablename) : list of its fields\n",
      "    rtFieldInfoThisFile = {}\n",
      "    for fieldInfo in allVarsThisFileSorted:\n",
      "        recordTag = fieldInfo['RecordTypeValue']\n",
      "        # do it for this file specifically\n",
      "        if recordTag not in rtFieldInfoThisFile:\n",
      "            rtFieldInfoThisFile[recordTag] = []\n",
      "        rtFieldInfoThisFile[recordTag].append(fieldInfo)\n",
      "        \n",
      "        if doGlobalOutput:\n",
      "           # build one across all files that contain this record type\n",
      "           # requires lotsa memory!\n",
      "            if recordTag not in rtFieldInfoAllFiles:\n",
      "                rtFieldInfoAllFiles[recordTag] = {}\n",
      "            fieldName = fieldInfo['Name']\n",
      "            if fieldName not in rtFieldInfoAllFiles[recordTag]:\n",
      "                rtFieldInfoAllFiles[recordTag][fieldName] = fieldInfo\n",
      "            else:\n",
      "                # the position of a given field in the line has to be the same each time\n",
      "                assert rtFieldInfoAllFiles[recordTag][fieldName]['Start'] == fieldInfo['Start']\n",
      "                assert rtFieldInfoAllFiles[recordTag][fieldName]['Len'] == fieldInfo['Len']\n",
      "\n",
      "    # Now parse the survey data file itself\n",
      "    # read the data and put everything into a list, in a dictionary keyed by record type\n",
      "    res = {}\n",
      "    colsPerRecTypeThisFile = {}\n",
      "    with open(datFN, 'r') as data:\n",
      "        for line in data:\n",
      "            # the position of the recordtype in the line is the same across the entire file\n",
      "            recordtype = line[rtStart:rtEnd]\n",
      "            # get the spec for this type of row\n",
      "            if recordtype not in rtFieldInfoThisFile:\n",
      "                print \"Specification for recordtype '{0!s}' not found in file for {1!s}\".format(\n",
      "                    recordtype, filecode)\n",
      "                continue\n",
      "            recordSpec = rtFieldInfoThisFile[recordtype]\n",
      "            if recordtype not in res:\n",
      "                res[recordtype] = []\n",
      "            \n",
      "            # split the column-aligned text according to the row specification\n",
      "            # this is the part that is inefficient in FME (lots of list items)\n",
      "            \n",
      "            # The .DAT format allows a fixed width for each column of each recordtype.\n",
      "            # Should we strip the whitespace? This is difficult. In general, yes we should \n",
      "            # but NOT in the case of the CASEID / HHID variables. The HHID is usually the CASEID\n",
      "            # with the last 3 chars trimmed off, but if we trim \"some\" whitespace from HHID here \n",
      "            # then we can break that association and damage referential integrity.\n",
      "            # On the other hand some joins are based on e.g. BIDX (len 2) to MIDX (len 1) and we need \n",
      "            # to join on a single digit found in both, so BIDX would need to be stripped.\n",
      "            stripornot = lambda data,name: data if name in ('CASEID', 'HHID') else data.strip()\n",
      "            rowParts = [stripornot(\n",
      "                    (line[i['Start']-1 : i['Start']+i['Len']-1]),\n",
      "                    i['Name']) \n",
      "                for i in recordSpec]\n",
      "            #rowParts = [(line[i['Start']-1 : i['Start']+i['Len']-1]).strip() for i in recordSpec]\n",
      "            #rowParts = [(line[i['Start']-1 : i['Start']+i['Len']-1]) for i in recordSpec]\n",
      "            if recordtype not in colsPerRecTypeThisFile:\n",
      "                colsPerRecTypeThisFile[recordtype] = len(rowParts)\n",
      "            else:\n",
      "                assert len(rowParts) == colsPerRecTypeThisFile[recordtype]\n",
      "            # add as a list to the list of rows for this record type\n",
      "            res[recordtype].append(rowParts)#(\",\".join(rowParts))\n",
      "    if doGlobalOutput:\n",
      "        resGlobal[filecode] = res\n",
      "\n",
      "    # write a csv for each record type\n",
      "    for recordtype,fields in rtFieldInfoThisFile.iteritems():\n",
      "        if not recordtype in res:\n",
      "            print \"No rows found for record type {0!s} in file {1!s} despite DCF specification\".format(\n",
      "                recordtype, filecode)\n",
      "            continue\n",
      "        fieldHeader = [i['Name'] for i in fields]\n",
      "        fieldRecords = set([i['RecordName'] for i in fields])\n",
      "        assert len(fieldRecords) == 1\n",
      "        recName = fieldRecords.pop()\n",
      "        outFN = os.path.join(outputCSVTableDir, '{0!s}.{1!s}.csv'.format(filecode,recName))\n",
      "        with open(outFN, 'wb') as outcsv:\n",
      "            csvwriter = csv.writer(outcsv)\n",
      "            csvwriter.writerow(fieldHeader)\n",
      "            csvwriter.writerows(res[recordtype])\n",
      "\n",
      "if doGlobalOutput:\n",
      "    # write one file for each table. Each output table must have the unioned set of columns \n",
      "    # from each survey's version of this table.\n",
      "    for filecode, fileres in resGlobal.iteritems():\n",
      "        for recordtype,fields in rtFieldInfoAllFiles.iteritems():\n",
      "            allFieldsThisRecordSorted = sorted(fields, key=(itemgetter('Start')))\n",
      "            groupFieldHeader = [i['Name'] for i in allFieldsThisRecordSorted]\n",
      "            fieldRecords = set([i['RecordName'] for i in fields])\n",
      "            assert len(fieldRecords) == 1\n",
      "            recName = fieldRecords.pop()\n",
      "            outGroupFN = os.path.join(outputCSVTableDir, '{0!s}_All.csv'.format(recName))\n",
      "            if not os.path.exists(outGroupFN):\n",
      "                # create the output file for this table\n",
      "                with open(outGroupFN, 'wb') as outcsv:\n",
      "                    csvwriter = csv.writer(outcsv)\n",
      "                    groupFieldHeader.insert(0,'FileCode')\n",
      "                    groupFieldHeader = [i.lower() for i in groupFieldHeader]\n",
      "                    csvwriter.writerow(groupFieldHeader)\n",
      "                    colsPerRecType[recordtype]=len(groupFieldHeader)\n",
      "               \n",
      "            [i.insert(0,filecode) for i in fileres[recordtype]]\n",
      "            with open(outGroupFN, 'ab') as outcsv:\n",
      "                csvwriter = csv.writer(outcsv)\n",
      "                resThisFileThisRecord = fileres[recordtype]\n",
      "                allSameLength = True\n",
      "                for i in resThisFileThisRecord:\n",
      "                    if len(i) != groupFieldHeader:\n",
      "                        allSameLength = False\n",
      "                        break\n",
      "                if allSameLength:\n",
      "                    csvwriter.writerows(resThisFileThisRecord)\n",
      "                else:\n",
      "                    for i in res:\n",
      "                        pass\n",
      "                        # Todo - write this \n",
      "\n",
      "                if len(res[recordtype][0]) != colsPerRecType[recordtype]:\n",
      "                    print (\"Warning! File {0!s} record type {1!s} has more cols than were defined in an earlier file\"\n",
      "                           .format(filecode, recordtype))\n",
      "            \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}